{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a02fdc-bd82-4447-811e-5cca667e48f6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import msal\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dca250-1941-4fdc-ac25-e9816885154d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ----------- CONFIGURATION ----------- #\n",
    "client_id = '<YOUR_CLIENT_ID>'\n",
    "client_secret = '<YOUR_CLIENT_SECRET>'\n",
    "tenant_id = '<YOUR_TENANT_ID>'\n",
    "workspace_id = '<YOUR_WORKSPACE_ID>'\n",
    "\n",
    "authority = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "scope = [\"https://analysis.windows.net/powerbi/api/.default\"]\n",
    "api_base = \"https://api.powerbi.com/v1.0/myorg\"\n",
    "api_base1 = \"https://api.fabric.microsoft.com/v1/workspaces\"\n",
    "#\"https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks\"\n",
    "\n",
    "# ----------- AUTHENTICATION ----------- #\n",
    "app = msal.ConfidentialClientApplication(\n",
    "    client_id, authority=authority, client_credential=client_secret\n",
    ")\n",
    "token_result = app.acquire_token_for_client(scopes=scope)\n",
    "access_token = token_result['access_token']\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "fabric_artifacts = {\"notebooks\", \"dataPipelines\"}\n",
    "\n",
    "# ----------- HELPER FUNCTION ----------- #\n",
    "def get_artifacts(endpoint, artifact_type):\n",
    "    if artifact_type in fabric_artifacts:\n",
    "        response = requests.get(f\"{api_base1}/{workspace_id}/{endpoint}\", headers=headers)\n",
    "    else:\n",
    "        response = requests.get(f\"{api_base}/groups/{workspace_id}/{endpoint}\", headers=headers)\n",
    "\n",
    "    response.raise_for_status()\n",
    "    data = response.json().get('value', [])\n",
    "\n",
    "    if artifact_type in fabric_artifacts:\n",
    "        return [\n",
    "            {\n",
    "                \"artifact_type\": artifact_type,\n",
    "                \"name\": item.get(\"displayName\"),\n",
    "                \"id\": item.get(\"id\"),\n",
    "                \"webUrl\": item.get(\"webUrl\", \"\"),\n",
    "                \"createdBy\": item.get(\"createdBy\", {}).get(\"email\", \"\")\n",
    "            } \n",
    "            for item in data\n",
    "            ]\n",
    "    else:    \n",
    "        return [\n",
    "            {\n",
    "                \"artifact_type\": artifact_type,\n",
    "                \"name\": item.get(\"name\"),\n",
    "                \"id\": item.get(\"id\"),\n",
    "                \"webUrl\": item.get(\"webUrl\", \"\"),\n",
    "                \"createdBy\": item.get(\"createdBy\", {}).get(\"email\", \"\")\n",
    "            }\n",
    "            for item in data\n",
    "        ]\n",
    "\n",
    "# ----------- FETCH ARTIFACTS ----------- #\n",
    "all_artifacts = []\n",
    "all_artifacts += get_artifacts(\"datasets\", \"Dataset\")\n",
    "all_artifacts += get_artifacts(\"reports\", \"Report\")\n",
    "all_artifacts += get_artifacts(\"dashboards\", \"Dashboard\")\n",
    "all_artifacts += get_artifacts(\"dataflows\", \"Dataflow\")\n",
    "all_artifacts += get_artifacts(\"notebooks\", \"notebooks\")\n",
    "all_artifacts += get_artifacts(\"dataPipelines\", \"dataPipelines\")\n",
    "# all_artifacts += get_artifacts(\"scorecards\", \"Scorecard\")  # optional\n",
    "\n",
    "# ----------- CREATE DATAFRAME ----------- #\n",
    "df_artifacts = pd.DataFrame(all_artifacts)\n",
    "\n",
    "# ----------- DISPLAY DATA ----------- #\n",
    "print(df_artifacts)\n",
    "spark_df = spark.createDataFrame(df_artifacts)\n",
    "spark_df = spark.createDataFrame(df_artifacts.astype(str))\n",
    "spark_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"power_bi_artifacts\")\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
